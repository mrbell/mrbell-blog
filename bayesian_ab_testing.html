<!DOCTYPE html>
<html lang="en"
>
<head>
    <title>Bayesian A/B testing with confidence - Michael Bell</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="http://michaelryanbell.com/bayesian_ab_testing.html">

        <meta name="author" content="Michael Bell" />
        <meta name="keywords" content="statistics,data" />
        <meta name="description" content="I have been developing the A/B testing procedure that I want to use in an upcoming experiment at work. Generally speaking I&#39;m on team Bayes when it comes to statistical matters, and that&#39;s the approach that I will take in this case as well. In this (long) post I&#39;ll outline a method for Bayesian A/B testing that is largely built on some excellent blog posts by Evan Miller, Chris Stuccio, and David Robinson. In this post I&#39;ll summarize the results of Miller and Stuccio, ending up with a decision function for A/B testing based on the gain expected from making the change that is under consideration. I go a bit further than the referenced posts by deriving a measure of uncertainty in the expected gain that can be used to determine when a result is significant and overcome the &#34;peeking&#34; problem discussed in Robinson&#39;s post. Lastly I calculate the expected difference between the two procedures under test (as opposed to the gain which is the difference but only when the new procedure is better). Throughout I present some simulated examples to give a sense of the impact of the different parameters in the model and to illustrate some aspects of the model that one should be aware of." />

        <meta property="og:site_name" content="Michael Bell" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Bayesian A/B testing with confidence"/>
        <meta property="og:url" content="http://michaelryanbell.com/bayesian_ab_testing.html"/>
        <meta property="og:description" content="I have been developing the A/B testing procedure that I want to use in an upcoming experiment at work. Generally speaking I&#39;m on team Bayes when it comes to statistical matters, and that&#39;s the approach that I will take in this case as well. In this (long) post I&#39;ll outline a method for Bayesian A/B testing that is largely built on some excellent blog posts by Evan Miller, Chris Stuccio, and David Robinson. In this post I&#39;ll summarize the results of Miller and Stuccio, ending up with a decision function for A/B testing based on the gain expected from making the change that is under consideration. I go a bit further than the referenced posts by deriving a measure of uncertainty in the expected gain that can be used to determine when a result is significant and overcome the &#34;peeking&#34; problem discussed in Robinson&#39;s post. Lastly I calculate the expected difference between the two procedures under test (as opposed to the gain which is the difference but only when the new procedure is better). Throughout I present some simulated examples to give a sense of the impact of the different parameters in the model and to illustrate some aspects of the model that one should be aware of."/>
        <meta property="article:published_time" content="2015-12-27" />
            <meta property="article:section" content="Blog" />
            <meta property="article:tag" content="statistics" />
            <meta property="article:tag" content="data" />
            <meta property="article:author" content="Michael Bell" />


    <!-- Bootstrap -->
        <link rel="stylesheet" href="http://michaelryanbell.com/theme/css/bootstrap.simplex.min.css" type="text/css"/>
    <link href="http://michaelryanbell.com/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="http://michaelryanbell.com/theme/css/pygments/native.css" rel="stylesheet">
    <link rel="stylesheet" href="http://michaelryanbell.com/theme/css/style.css" type="text/css"/>

        <link href="http://michaelryanbell.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Michael Bell ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="http://michaelryanbell.com/" class="navbar-brand">
Michael Bell            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                        <li class="active">
                            <a href="http://michaelryanbell.com/category/blog.html">Blog</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="http://michaelryanbell.com/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">

    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="http://michaelryanbell.com/bayesian_ab_testing.html"
                       rel="bookmark"
                       title="Permalink to Bayesian A/B testing with confidence">
                        Bayesian A/B testing with confidence
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2015-12-27T00:00:00-05:00"> Sun 27 December 2015</time>
    </span>



<span class="label label-default">Tags</span>
	<a href="http://michaelryanbell.com/tag/statistics.html">statistics</a>
        /
	<a href="http://michaelryanbell.com/tag/data.html">data</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p><strong>UPDATE (12/30/2015)</strong>: In response to a comment by Niels Oppermann, I have added
derivations of the expected difference <span class="math">\(p_B-p_A\)</span> in addition to the
expected gain <span class="math">\(\mathrm{max}\left(p_B-p_A, 0\right)\)</span> (i.e. the difference when
<span class="math">\(p_B&gt;p_A\)</span>).</p>


<p>I have been developing the A/B testing procedure that I want to use in
an upcoming experiment at work. Generally speaking I'm on team Bayes when it comes
to statistical matters, and that's the approach that I will take in this case as well. In
this (long) post I'll outline a method for Bayesian A/B testing that is largely built on some
excellent blog posts by <a href="http://www.evanmiller.org/bayesian-ab-testing.html">Evan Miller</a>,
<a href="https://www.chrisstucchio.com/blog/2014/bayesian_ab_decision_rule.html">Chris Stuccio</a>,
and <a href="http://varianceexplained.org/r/bayesian-ab-testing/">David Robinson</a>.
In this post I'll summarize the results of Miller and Stuccio, ending up with a
decision function for A/B testing based on the gain expected from making the
change that is under consideration. I go a bit further than the referenced posts
by deriving a measure of uncertainty in the expected gain that can be used to
determine when a result is significant and overcome the "peeking" problem discussed
in Robinson's post. Lastly I calculate the expected difference between the two
procedures under test (as opposed to the gain which is the difference but
only when the new procedure is better).
Throughout I present some simulated examples to give a sense of the impact of the
different parameters in the model and to illustrate some aspects of the model
that one should be aware of.</p>


<h2>The basic model</h2>
<p>In an A/B test, one has two different procedures (i.e. treatments) under consideration
and would like to know which one produces the best results. For example, an e-commerce
company might have a couple of different web site layouts that they are considering
and would like to know which one has the best click through rate to a product page.
An experiment is run where two groups are selected at random from the overall
population. The control group A uses the standard procedure and the test group B
uses the new procedure. At the end of the experiment the data is analyzed to
decide whether the outcome for group B was better (or worse, or the same)
than the outcome for group A.</p>
<p>In the simplest case, which I consider here, the outcome is binary,
e.g. either a customer clicked through to a product page or they didn't.
In this case, the most natural choice for the <em>likelihood</em>, i.e. the
probability that a number of positive outcomes <em>k</em> is observed from a total
of <em>n</em> trials with probability of success <em>p</em> per trial, is the Binomial distribution</p>
<div class="math">\begin{equation}P(k|n,p)=\mathrm{Binomial}(k;n,p)=\left(\begin{array}{c}n\\k\end{array}\right)p^{k}\left(1-p\right)^{n-k}.\end{equation}</div>
<p>Of course <em>n</em> and <em>k</em> can be measured experimentally, but <em>p</em> must be inferred.
It's time for Bayes' theorem (the important bit of it anyway)</p>
<div class="math">\begin{equation}P\left(p|k,n\right)\propto P(k|n,p)P(p)\end{equation}</div>
<p>where <span class="math">\(P(p)\)</span> describes any prior knowledge about <em>p</em>. A convenient choice of
prior distribution is the Beta distribution which is conjugate to the Binomial
distribution</p>
<div class="math">\begin{equation}P(p)=\mathrm{Beta}(p;\alpha,\beta)=\frac{p^{\alpha-1}(1-p)^{\beta-1}}{\mathrm{B}(\alpha,\beta)}\end{equation}</div>
<p>where <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span> are parameters of the Beta distribution and
<span class="math">\(\mathrm{B}(\alpha,\beta)\)</span> is the <a href="https://en.wikipedia.org/wiki/Beta_function">Beta function</a>.
The parameters <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span> can be set empirically from historical data,
using hierarchical modeling, or chosen manually. For instance, a uniform prior
on the interval [0,1] can be chosen by setting <span class="math">\(\alpha=1\)</span> and <span class="math">\(\beta=1\)</span>.</p>
<p>A Binomial likelihood and Beta prior leads to a posterior distribution that is also Beta
distributed</p>
<div class="math">\begin{equation}P\left(p|k,n\right)=p^{k+\alpha-1}\left(1-p\right)^{n-k+\beta-1}=\mathrm{Beta}(p;\alpha+k,n-k+\beta)=\mathrm{Beta}(p;\alpha',\beta')\end{equation}</div>
<p>where <span class="math">\(\alpha'=\alpha+k\)</span> is the prior parameter <span class="math">\(\alpha\)</span> plus the number of
successful trials and <span class="math">\(\beta'=n-k+\beta\)</span> is the prior parameter
<span class="math">\(\beta\)</span> plus the number of failed trials.</p>
<p>The figure below shows the posterior distributions for <span class="math">\(p_{A}\)</span> and <span class="math">\(p_{B}\)</span>
for a simulated data set with <span class="math">\(n=500\)</span> observations. I have chosen a true <span class="math">\(p_{A}\)</span> of 0.5, a true <span class="math">\(p_{B}\)</span>
of 0.55, and a prior distribution such that p is expected to most likely fall
between 0.4 and 0.6 with a mean of 0.5. The posterior mean approaches
the ratio <span class="math">\(k/n\)</span> in each case, but there is still some influence of the prior
given the limited amount of data. With more and more data, the posterior
distribution will become centered on the ratio <span class="math">\(k/n\)</span> and increasingly more narrow.</p>
<p><img alt="Example Posterior Inference" src="http://michaelryanbell.com/images/example_posterior_inference.png" /></p>
<h2>The probability that the new procedure is better</h2>
<p>Given the experimental data and the posterior distributions for p, how does one know
that the new procedure is better than the old one i.e. that <span class="math">\(p_{B}&gt;p_{A}\)</span>? If you
were looking at a plot like the one above after an experiment, with the max of <span class="math">\(P(p_B | n_B, k_B)\)</span>
clearly higher than that of <span class="math">\(P(p_A | n_A, k_A)\)</span> but some overlap in the distributions, what
would you do? How confident are you that the new treatment is better than the
baseline? How much better is it?</p>
<p>To answer these questions I follow the approach outlined in
<a href="http://www.evanmiller.org/bayesian-ab-testing.html">a blog post by Evan Miller</a>,
namely to calculate the probability <span class="math">\(P\left(p_{B}&gt;p_{A}\right)\)</span>, which is</p>
<div class="math">\begin{align}P\left(p_{B}&gt;p_{A}\right)&amp;=\int_{0}^{1}\int_{p_{A}}^{1}dp_{A}dp_{B}\frac{p_{A}^{\alpha'_{A}-1}(1-p_{A})^{\beta'_{A}-1}}{\mathrm{B}(\alpha'_{A},\beta'_{A})}\frac{p_{B}^{\alpha'_{B}-1}(1-p_{B})^{\beta'_{B}-1}}{\mathrm{B}(\alpha'_{B},\beta'_{B})}\\
&amp;=\sum_{i=0}^{\alpha'_{B}-1}\frac{\mathrm{B}\left(\alpha'_{A}+i,\beta'_{A}+\beta'_{B}\right)}{\left(\beta'_{B}+i\right)\mathrm{B}\left(1+i,\beta'_{B}\right)\mathrm{B}\left(\alpha_{A},\beta_{A}\right)}\\
&amp;=h(\alpha'_{A},\beta'_{A},\alpha'_{B},\beta'_{B}).\end{align}</div>
<p>See the original post for a detailed derivation. I'll call this quantity the
<em>probability of improvement (POI)</em>. Note that I'm generally interested
in the probability that the new procedure, applied to group B, is <em>better</em> than the
standard procedure applied to group A. I could alternatively consider how likely
it is that it's worse and I'm making a mistake by adopting the new procedure.
In this case I simply swap the A and B labels above.</p>
<p>The POI is an interesting metric, but it's not ideally suited for making decisions.
It indicates only that one procedure is better than another, but not by how much.
A marginal improvement may not justify a change
to the current procedure, so if B is only slightly better than A, I would like to know.
Actually, in case the two procedures are indeed the same then the POI turns out to be a very unreliable metric.</p>
<p>The figure below shows the range of POIs from 100 simulated experiments as calculated every 1000 trials
(e.g. page views for the e-commerce example) up to 80,000 trials.
In this case the true values of <span class="math">\(p_A\)</span> and <span class="math">\(p_B\)</span> are both 0.5.
I show the average value from all 100 simulations at each value of <span class="math">\(n\)</span> together with the
intervals covering 65% and 95% of all observed values.</p>
<p><img alt="POI with equal probabilities for each group" src="http://michaelryanbell.com/images/poi_equal_probabilities.png" /></p>
<p>The simulations show that nearly any POI can be observed
when the two procedures truly have the same effect. Judging solely on POI,
there is a decent chance that one could observe <span class="math">\(p_B\)</span> to be greater than <span class="math">\(p_A\)</span>
when in fact it is not. What is surprising to me is that the spread of results
never narrows. Running a longer test wont reduce the chance that a high POI
will be reported.</p>
<h2>How much better is the new procedure?</h2>
<p>So POI isn't a great metric to base decisions on. Instead, I will
use the <em>expected gain</em> to evaluate our results as described <a href="https://www.chrisstucchio.com/blog/2014/bayesian_ab_decision_rule.html">in a blog post by Chris Stuccio</a>.
Note that the referenced article actually considers the expected loss. Again I am interested
in calculating how much better the new procedure is from the baseline, not whether
it is worse, so I'll work with the expected gain. Again, the difference is in
the order of the B and A labels in the expressions below.</p>
<p>The expected gain extends the concept of POI to not only include the probability
that one outcome is better than the other,
but also how much better the outcome will be. The metric is defined as
<span class="math">\(P\left(\mathrm{max}\left[p_B - p_A, 0\right]\right)\)</span></p>
<div class="math">\begin{align}P\left(\mathrm{max}\left[p_B-p_A,0\right]\right)&amp;=\int_{0}^{1}\int_{0}^{1}dp_{A}dp_{B}\mathrm{max}\left[\left(p_{B}-p_{A}\right),\,0\right]\frac{p_{A}^{\alpha'_{A}-1}(1-p_{A})^{\beta'_{A}-1}}{\mathrm{B}(\alpha'_{A},\beta'_{A})}\frac{p_{B}^{\alpha'_{B}-1}(1-p_{B})^{\beta'_{B}-1}}{\mathrm{B}(\alpha'_{B},\beta'_{B})}\\
&amp;=\int_{0}^{1}\int_{p_{A}}^{1}dp_{A}dp_{B}\left(p_{B}-p_{A}\right)\frac{p_{A}^{\alpha'_{A}-1}(1-p_{A})^{\beta'_{A}-1}}{\mathrm{B}(\alpha'_{A},\beta'_{A})}\frac{p_{B}^{\alpha'_{B}-1}(1-p_{B})^{\beta'_{B}-1}}{\mathrm{B}(\alpha'_{B},\beta'_{B})}\\
&amp;=\frac{\mathrm{B}(\alpha'_{B}+1,\beta'_{B})}{\mathrm{B}(\alpha'_{B},\beta'_{B})}h(\alpha'_{A},\beta'_{A},\alpha'_{B}+1,\beta'_{B})-\frac{\mathrm{B}(\alpha'_{A}+1,\beta'_{A})}{\mathrm{B}(\alpha'_{A},\beta'_{A})}h(\alpha'_{A}+1,\beta'_{A},\alpha'_{B},\beta'_{B})\\
&amp;=\frac{\alpha'_{B}}{\alpha'_{B}+\beta'_{B}}h(\alpha'_{A},\beta'_{A},\alpha'_{B}+1,\beta'_{B})-\frac{\alpha'_{A}}{\alpha'_{A}+\beta'_{A}}h(\alpha'_{A}+1,\beta'_{A},\alpha'_{B},\beta'_{B})\end{align}</div>
<p>where I have used the property of the Beta function</p>
<div class="math">\begin{equation}\mathrm{B}\left(\alpha+1,\beta\right)=\mathrm{B}\left(\alpha,\beta\right)\frac{\alpha}{\alpha+\beta}.\end{equation}</div>
<p>The expected gain can be used to evaluate the outcome of an experiment. When reporting
to decision makers, this outcome not only says that B turned out better than A, but
it also provides an estimate of how much better it is.</p>
<p>What happens to the expected gain when the true values of <span class="math">\(p_B\)</span> and <span class="math">\(p_A\)</span> are equal?
The POI wasn't always reliable in this situation. Is the gain a better metric?
I'll turn again to simulated experimental results to find out.</p>
<p><img alt="Expected gain with equal probabilities for each group" src="http://michaelryanbell.com/images/gain_equal_probabilities.png" /></p>
<p>It's clear that the gain behaves much better than the POI did above. The gain
quickly falls to a small value relatively quickly, and drops closer to the
true value of 0 as the experiment continues. When looking for, say, a
1% gain from a new procedure, this metric would rightly indicate that such a
gain was not likely.</p>
<h2>On "peeking" and uncertainty in the gain</h2>
<p>It has been suggested that one can "peek" at the gain throughout the experiment
to watch for it to go above some threshold value, but
<a href="http://varianceexplained.org/r/bayesian-ab-testing/">this post by David Robinson</a>
shows through a series of simulations that this practice leads to an excessive rate of false positives
(although the excessive false positive rate is not as extreme as if one were
peeking at a <em>p</em>-value).</p>
<p>I'll give an example. Say we are running an experiment
and are interested in implementing a new procedure (applied to group B) if we find a 1% gain
in <span class="math">\(p_B\)</span> over <span class="math">\(p_A\)</span>. But let's imagine that <span class="math">\(p_B\)</span> is actually 1% <em>lower</em> than
<span class="math">\(p_A\)</span>.</p>
<p><img alt="Gain as a function of experiment length when B is worse than A." src="http://michaelryanbell.com/images/gain_B_is_worse.png" /></p>
<p>The figure above shows the expected gain from 100 simulated experiments where again
I "peek" at the gain every 1000 trials. Until about 5000 trials
there is a good chance that a 1% gain in B over A can be observed, and if I saw that
and decided to stop the experiment early declaring success I would be making a mistake. The
longer the experiment, the less likely it is that such a mistake would be made.
But if I were watching the data to see a 1% gain and stopped the experiment as
soon as I did, I would end up making a lot of bad decisions.</p>
<p>In the simulated examples above it is clear that with limited data, the variance
in the observed expected gain in repeated experiments is too large
and as a result reasonable decisions can't be made. How does one know when the result is "significant"?
If the variance was known to be large and therefore the estimate of expected gain was inaccurate, then it would
be obvious that more data was needed. Simulations can provide estimates of the
variance, as above, but the variance in <span class="math">\(\mathrm{max}\left[p_B - p_A, 0\right]=\delta_{BA}\)</span> can also be
calculated directly</p>
<div class="math">\begin{equation}P\left[\left(\delta_{BA}-\bar{\delta}_{BA}\right)^{2}\right]=\int_{0}^{1}\int_{p_{A}}^{1}dp_{A}dp_{B}\left(p_{B}-p_{A}-\bar{\delta}_{BA}\right)^{2}\frac{p_{A}^{\alpha'_{A}-1}(1-p_{A})^{\beta'_{A}-1}}{\mathrm{B}(\alpha'_{A},\beta'_{A})}\frac{p_{B}^{\alpha'_{B}-1}(1-p_{B})^{\beta'_{B}-1}}{\mathrm{B}(\alpha'_{B},\beta'_{B})}\end{equation}</div>
<p>The derivation proceeds very similarly to that of the expected gain</p>
<div class="math">\begin{align}P\left[\left(\delta_{BA}-\bar{\delta}_{BA}\right)^{2}\right]=&amp;\frac{\alpha'_{B}\left(\alpha'_{B}+1\right)}{\left(\alpha'_{B}+\beta'_{B}\right)\left(\alpha'_{B}+\beta'_{B}+1\right)}h(\alpha'_{A},\beta'_{A},\alpha'_{B}+2,\beta'_{B})\ldots\notag\\
&amp;+\frac{\alpha'_{A}\left(\alpha'_{A}+1\right)}{\left(\alpha'_{A}+\beta'_{A}\right)\left(\alpha'_{A}+\beta'_{A}+1\right)}h(\alpha'_{A}+2,\beta'_{A},\alpha'_{B},\beta'_{B})\ldots\notag\\
&amp;+\bar{\delta}_{BA}^{2}h(\alpha'_{A},\beta'_{A},\alpha'_{B},\beta'_{B})-2\frac{\alpha'_{A}}{\alpha'_{A}+\beta'_{A}}\frac{\alpha'_{B}}{\alpha'_{B}+\beta'_{B}}h(\alpha'_{A}+1,\beta'_{A},\alpha'_{B}+1,\beta'_{B})\ldots\notag\\
&amp;+2\bar{\delta}_{BA}\frac{\alpha'_{A}}{\alpha'_{A}+\beta'_{A}}h(\alpha'_{A}+1,\beta'_{A},\alpha'_{B},\beta'_{B})-2\bar{\delta}_{BA}\frac{\alpha'_{B}}{\alpha'_{B}+\beta'_{B}}h(\alpha'_{A},\beta'_{A},\alpha'_{B}+1,\beta'_{B}).\end{align}</div>
<p>The expected gain, together with it's variance given here, are well suited to
evaluating the experimental outcome. As an experiment is run
the expected gain and it's variance can be calculated. If the gain is found to be significant
<em>relative to</em> the square root of the variance, then the experiment can safely be stopped.</p>
<p><img alt="Average gain together with uncertainty compared to variance observed in experiments with 5% gain." src="http://michaelryanbell.com/images/gain_w_uncert.png" /></p>
<p>In the figure above, I show the results of another set of 100 simulations. In this
case the true value of <span class="math">\(p_B\)</span> is 5% higher than <span class="math">\(p_A\)</span>. The average
of all observed gains converges on the correct difference pretty quickly. I also
show the observed spread in gains over the 100 simulations as the interval
covering 68% of the observed values and
the 1-<span class="math">\(\sigma\)</span> uncertainty as estimated by the square root of the variance calculated
using the equation above. The closed form "1-<span class="math">\(\sigma\)</span>" uncertainty
calculation matches the spread of simulated results very closely. The figure
below is similar, but for a true <span class="math">\(p_B\)</span> that is only 1% higher than <span class="math">\(p_A\)</span> for
comparison.</p>
<p><img alt="Average gain together with uncertainty compared to variance observed in experiments with 1% gain." src="http://michaelryanbell.com/images/gain_w_uncert_1percent.png" /></p>
<h2>What's the difference?</h2>
<p><strong>Thanks to Niels Oppermann for motivating this section.</strong></p>
<p>The expected gain and its variance have both been calculated under the condition that
<span class="math">\(p_B&gt;p_A\)</span>. Instead, one could simply calculate the <em>expected difference</em>
<span class="math">\(p_B-p_A=\delta'_{BA}\)</span> without specifying that the new procedure outperforms the standard procedure</p>
<div class="math">\begin{align}P\left(p_B-p_A\right)&amp;=\int_{0}^{1}\int_{0}^{1}dp_{A}dp_{B}\left(p_{B}-p_{A}\right)\frac{p_{A}^{\alpha'_{A}-1}(1-p_{A})^{\beta'_{A}-1}}{\mathrm{B}(\alpha'_{A},\beta'_{A})}\frac{p_{B}^{\alpha'_{B}-1}(1-p_{B})^{\beta'_{B}-1}}{\mathrm{B}(\alpha'_{B},\beta'_{B})}\notag\\
&amp;=\frac{\mathrm{B}(\alpha'_{B}+1,\beta'_{B})}{\mathrm{B}(\alpha'_{B},\beta'_{B})}-\frac{\mathrm{B}(\alpha'_{A}+1,\beta'_{A})}{\mathrm{B}(\alpha'_{A},\beta'_{A})}\notag\\
&amp;=\frac{\alpha'_{B}}{\alpha'_{B}+\beta'_{B}}-\frac{\alpha'_{A}}{\alpha'_{A}+\beta'_{A}}\label{eq:exp_diff}\\
&amp;=\bar{\delta}'_{BA}\label{eq:exp_diff2}\end{align}</div>
<p>This is just the difference between the posterior means for <span class="math">\(p_B\)</span> and <span class="math">\(p_A\)</span> as
one might intuitively expect. What about the variance? The derivation is similar
to the variance in the gain and the result is similar except a bit cleaner without the
<span class="math">\(h(\alpha_A, \beta_A, \alpha_B, \beta_B)\)</span> terms.</p>
<div class="math">\begin{align}P\left[\left(\delta'_{BA}-\bar{\delta}'_{BA}\right)^{2}\right]=&amp;\frac{\alpha'_{B}\left(\alpha'_{B}+1\right)}{\left(\alpha'_{B}+\beta'_{B}\right)\left(\alpha'_{B}+\beta'_{B}+1\right)}+\frac{\alpha'_{A}\left(\alpha'_{A}+1\right)}{\left(\alpha'_{A}+\beta'_{A}\right)\left(\alpha'_{A}+\beta'_{A}+1\right)}\ldots\notag\\
&amp;-2\frac{\alpha'_{A}}{\alpha'_{A}+\beta'_{A}}\frac{\alpha'_{B}}{\alpha'_{B}+\beta'_{B}}-\bar{\delta}_{BA}^{2}.\label{eq:var_diff}\end{align}</div>
<p>The variance in the expected difference consists of the sum of two terms that are very nearly
- but interestingly not quite - the posterior variances of <span class="math">\(p_B\)</span> and <span class="math">\(p_A\)</span> minus
the square of the expected difference and a cross term.</p>
<p>In the figure below I show a comparison of the expected difference and gain for
a single simulated experiment including the "<span class="math">\(1\sigma\)</span>" uncertainties (i.e. plus or minus the
square root of the variance in each measure). In this case the true difference
between <span class="math">\(p_B\)</span> and <span class="math">\(p_A\)</span> is 1%. After about 5000 trials, the two measures are
essentially the same.</p>
<p><img alt="Comparison of expected gain and difference for a simulated example." src="http://michaelryanbell.com/images/gain_and_diff_1percent.png" /></p>
<p>When the gain or difference is larger, the two quantities converge even more quickly.
But what about when the difference is negative? In this case, there is much more discrepancy
between the gain and the difference because the gain is a strictly positive quantity
while the difference can take both positive and negative values.</p>
<p><img alt="Comparison of expected gain and difference for a simulated example with negative difference." src="http://michaelryanbell.com/images/gain_and_diff_neg1percent.png" /></p>
<p>While I started by providing the gain and it's variance to build on the work of the blog posts
of Miller and Stuccio, I will actually opt to use the expected difference and it's
variance in my experiments.</p>
<h2>Conclusions</h2>
<p>So finally I have everything that I'll need to run basic experiments in situations
with binary outcomes. The expected difference (eq.<span class="math">\(\,\ref{eq:exp_diff3}\)</span>),
together with it's variance (eq.<span class="math">\(\,\ref{eq:var_diff}\)</span>), are enough to indicate not only how much better one
might do by switching to a new procedure, but also how much one should trust the
result. Experimental results can be monitored regularly as long as both the
difference and variance are evaluated at each step, and it's fine to
stop the experiment if the difference is found to be good enough when compared to the
uncertainty.</p>
<p>These results apply to two procedure (A/B) tests where outcomes are binary. Similar
results can be obtained when testing 3 or more procedures in parallel or if
the outcomes are not binary but e.g. counts or rates instead. For example,
<a href="http://www.evanmiller.org/bayesian-ab-testing.html">Evan Miller</a> calculates POI
for count data and for three test groups. An expected difference and it's variance could
be calculated in these cases as well.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'michaelryanbellcom'; // required: replace example with your forum shortname

                    var disqus_identifier = 'bayesian_ab_testing';
                var disqus_url = 'http://michaelryanbell.com/bayesian_ab_testing.html';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>

<section class="well well-sm">
    <ul class="list-group list-group-flush">
            <li class="list-group-item"><h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
              <ul class="list-group" id="social">
                <li class="list-group-item"><a href="https://twitter.com/mryanbell"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
                <li class="list-group-item"><a href="https://github.com/mrbell"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
              </ul>
            </li>



            <li class="list-group-item"><a href="http://michaelryanbell.com/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
                <ul class="list-group " id="tags">
                </ul>
            </li>
    </ul>
</section>
            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2015 Michael Bell
            &middot; Powered by <a href="https://github.com/DandyDev/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://michaelryanbell.com/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="http://michaelryanbell.com/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="http://michaelryanbell.com/theme/js/respond.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'michaelryanbellcom'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-59163082-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->

</body>
</html>